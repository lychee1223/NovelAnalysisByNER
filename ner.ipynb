{"cells":[{"cell_type":"markdown","metadata":{"id":"U-QsosDwgKiB"},"source":["## ライブラリをインポート"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Google Colab で実行する場合は以下のライブラリをインストール\n","# !pip install fugashi ipadic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrVUrLpeWf4a"},"outputs":[],"source":["import os\n","import re\n","import unicodedata\n","import itertools\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertJapaneseTokenizer, BertForTokenClassification"]},{"cell_type":"markdown","metadata":{},"source":["## パラメータの設定"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SEED_VALUE = 42\n","MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'\n","MAX_LENGTH = 128\n","TRAIN_BATCH_SIZE = 16\n","TEST_BATCH_SIZE = 256\n","MAX_EPOCH = 5\n","LEARNING_RATE = 2e-5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def set_seed(seed=SEED_VALUE):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","# シード値の固定\n","set_seed()"]},{"cell_type":"markdown","metadata":{"id":"XFm74RE2TbW8"},"source":["## データセットのダウンロード"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMu6u7yhWhVB"},"outputs":[],"source":["# データのダウンロード\n","if not os.path.exists('ner-wikipedia-dataset'):\n","    !git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset\n","\n","# データのロード\n","df = pd.read_json('ner-wikipedia-dataset/ner.json')"]},{"cell_type":"markdown","metadata":{"id":"BYW0l5-BHnxN"},"source":["## 前処理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGpumixYgfKR"},"outputs":[],"source":["# 固有表現タイプの辞書\n","id_dict = {'人名': 1,\n","           '法人名': 2,\n","           '政治的組織名': 3,\n","           'その他の組織名': 4,\n","           '地名': 5,\n","           '施設名': 6,\n","           '製品名': 7,\n","           'イベント名': 8}\n","\n","# idからtypeを取得する関数\n","def get_type_from_id(id):\n","    keys = [key for key, value in id_dict.items() if value == id]\n","    if keys:\n","        return keys[0]\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnKbc0BWkN_"},"outputs":[],"source":["def preprocess(data):\n","  # ｱｲｳ → アイウ, ＡＢＣ → ABC, １２３ → 123\n","  data['text'] = unicodedata.normalize('NFKC', data['text'])\n","\n","  # typeを対応するtype_idに変換\n","  for entity in data['entities']:\n","    entity['type_id'] = id_dict[entity['type']]\n","\n","  return data\n","\n","df = df.apply(preprocess, axis=1)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZZpntXhWl-W"},"outputs":[],"source":["# データセットの分割 (学習:検証:テスト = 6:2:2)\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, )\n","train_df, valid_df = train_test_split(train_df, test_size=0.25, random_state=42)\n","\n","# index降り直し\n","train_df.reset_index(drop=True, inplace=True)\n","valid_df.reset_index(drop=True, inplace=True)\n","test_df.reset_index(drop=True, inplace=True)\n","\n","# サイズ表示\n","print(f'学習データ数   : {len(train_df)}')\n","print(f'検証データ数   : {len(valid_df)}')\n","print(f'テストデータ数 : {len(test_df)}')"]},{"cell_type":"markdown","metadata":{"id":"QuvStWuPfhFj"},"source":["## トークナイザの準備"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uykbaGWD4Z9"},"outputs":[],"source":["'''固有表現抽出に適応したBertJapaneseTokenizerを拡張したトークナイザ\n","'''\n","class ExtensionTokenizer(BertJapaneseTokenizer):\n","\n","    # 学習時に用いるラベル付きエンコーダ\n","    def tagged_encode_plus(self, text, entity_list, max_length):\n","        '''[Step1] 固有表現かそれ以外かで分割\n","        '''\n","        entity_list = sorted(entity_list, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n","\n","        data_splitted = [] # 分割後の文字列格納用\n","        head = 0           # 文字列の先頭のindex\n","\n","        for entity in entity_list:\n","            # 次に出現する固有表現の先頭・末尾・IDを取得\n","            entity_head = entity['span'][0]\n","            entity_tail = entity['span'][1]\n","            label = entity['type_id']\n","\n","            # 固有表現にID、固有表現以外に'0'をラベルとして付与\n","            data_splitted.append({'text': text[head:entity_head], 'label':0})\n","            data_splitted.append({'text': text[entity_head:entity_tail], 'label':label})\n","\n","            head = entity_tail  # 先頭indexを更新\n","\n","        # 最後の固有表現以降のtextに'0'をラベルとしてを付与\n","        data_splitted.append({'text': text[head:], 'label':0})\n","\n","        # head = entity_startの時、{'text': '', 'label': 0}となってしまうため、textが空の要素を削除\n","        data_splitted = [ s for s in data_splitted if s['text'] ]\n","\n","        '''[Step2] トークナイザを用い、分割された文字列をトークン化・ラベル付与\n","        '''\n","        tokens = []\n","        labels = []\n","\n","        for s in data_splitted:\n","            text_splitted = s['text']\n","            label_splitted = s['label']\n","\n","            tokens_splitted = self.tokenize(text_splitted)        # トークン化\n","            labels_splitted = [label_splitted] * len(tokens_splitted)  # 各トークンにラベル付与\n","\n","            tokens.extend(tokens_splitted)  # トークンを結合\n","            labels.extend(labels_splitted)  # ラベルを結合\n","\n","        '''[Step3] BERTに入力可能な形式に符号化\n","        '''\n","        encoding = self.encode_plus(tokens,\n","                                    max_length=max_length,\n","                                    padding='max_length',\n","                                    truncation=True,\n","                                    return_tensors='pt')\n","\n","        # トークン[CLS]、[SEP]に'0'ラベルとして付与\n","        labels = [0] + labels[:max_length-2] + [0]\n","        # トークン[PAD]に'0'をラベルとして付与\n","        labels = labels + [0]*( max_length - len(labels) )\n","\n","        encoding['input_ids'] = encoding['input_ids'][0]\n","        encoding['attention_mask'] = encoding['attention_mask'][0]\n","        encoding['token_type_ids'] = encoding['token_type_ids'][0]\n","        encoding['labels'] = torch.tensor([labels])[0]\n","        return encoding\n","\n","    # テスト時に用いるencordingとspansを返すエンコーダ\n","    def untagged_encode_plus(self, text, max_length):\n","        '''[Step1] BERTに入力可能な形式に符号化\n","        '''\n","        encoding = self.encode_plus(text=text,\n","                                    max_length=max_length,\n","                                    padding='max_length',\n","                                    truncation=True,\n","                                    return_tensors = 'pt')\n","\n","        encoding['input_ids'] = encoding['input_ids'][0]\n","        encoding['token_type_ids'] = encoding['token_type_ids'][0]\n","        encoding['attention_mask'] = encoding['attention_mask'][0]\n","\n","        '''[Step2]各トークンのスパンを格納\n","        '''\n","        spans = []\n","\n","        tokens = self.convert_ids_to_tokens(encoding['input_ids'])\n","        head = 0\n","\n","        for token in tokens:\n","            # '##'は文字数にカウントしないので読み飛ばす\n","            token = token.replace('##','')\n","\n","            # スペシャルトークンの場合はダミーとしてspanを[-1, -1]とする\n","            if token == '[PAD]':\n","                spans.append([-1, -1])\n","            elif token == '[UNK]':\n","                spans.append([-1, -1])\n","            elif token == '[CLS]':\n","                spans.append([-1, -1])\n","            elif token == '[SEP]':\n","                spans.append([-1, -1])\n","\n","            # text中からtokenをを探索し，開始位置 + 文字列長をspanとする\n","            # トークンが見つかるまでスペースを読み飛ばす\n","            else:\n","                length = len(token)\n","                while 1:\n","                    if token == text[head:head+length]:\n","                        spans.append([head, head+length])\n","                        head += length\n","                        break\n","\n","                    head += 1\n","\n","        spans = torch.tensor(spans)\n","        return encoding, spans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SB7eIkZJY4DI"},"outputs":[],"source":["# 拡張したトークナイザをロード\n","tokenizer = ExtensionTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"9S2aIhnligQH"},"source":["## データセット・データローダーの作成"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ye-H0eZVfyYp"},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, texts, entity_lists, tokenizer, max_length):\n","        self.texts = texts\n","        self.entity_lists = entity_lists\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        entity_list = self.entity_lists[index]\n","\n","        encoding = self.tokenizer.tagged_encode_plus(text, entity_list, self.max_length)\n","        input_ids = encoding['input_ids']\n","        attention_mask = encoding['attention_mask']\n","        labels = encoding['labels']\n","\n","        return text, input_ids, attention_mask, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFO5eLBdcrbS"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_length):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","\n","        encoding, spans = self.tokenizer.untagged_encode_plus(text, self.max_length)\n","        input_ids = encoding['input_ids']\n","        attention_mask = encoding['attention_mask']\n","\n","        return  text, input_ids, attention_mask, spans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Twms1gsSW9Ir"},"outputs":[],"source":["# データセットの作成\n","train_dataset = TrainDataset(texts = train_df['text'], entity_lists = train_df['entities'], tokenizer = tokenizer, max_length = MAX_LENGTH)\n","valid_dataset = TrainDataset(texts = valid_df['text'], entity_lists = valid_df['entities'], tokenizer = tokenizer, max_length = MAX_LENGTH)\n","test_dataset = TestDataset(texts = test_df['text'], tokenizer = tokenizer, max_length = MAX_LENGTH)\n","\n","# データローダの作成\n","train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, pin_memory=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True, pin_memory=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, pin_memory=True)"]},{"cell_type":"markdown","metadata":{"id":"FO1ynRvFTQZu"},"source":["## 事前学習モデル"]},{"cell_type":"markdown","metadata":{"id":"FBjjWttyumq5"},"source":["###モデルの準備\n","BERTをロードし、概要を確認する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JS5xRIBUTPra"},"outputs":[],"source":["# 学習済みモデルをロード\n","model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n","\n","print(f'\\nmodelのパラメータを確認:\\n{model.get_parameter}')"]},{"cell_type":"markdown","metadata":{"id":"kAHl4IU7FQ7h"},"source":["### 推論"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fl4tzMFBgu7"},"outputs":[],"source":["'''文字列の符号化、BERTによる推論、BERTの出力をentitiesに変換する関数\n","'''\n","def predict(test_dataloader, model):\n","    # モデルをGPUまたはCPUに乗せる\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    # 予測\n","    model.eval()\n","    entity_lists = []\n","    with torch.inference_mode():\n","        for batch_text, batch_input_ids, batch_attention_mask, batch_spans in tqdm(test_dataloader):\n","            batch_input_ids = batch_input_ids.to(device)\n","            batch_attention_mask = batch_attention_mask.to(device)\n","            output = model(input_ids = batch_input_ids, attention_mask = batch_attention_mask)\n","\n","            for logits, text, spans in zip(output.logits, batch_text, batch_spans):\n","                # 最も高い確率のクラスを予測ラベルとする\n","                labels = [logit.argmax(-1).cpu().item() for logit in logits]\n","\n","                # スペシャルトークンを削除\n","                labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n","                spans = [span for span in spans if span[0] != -1]\n","\n","                # 同じラベルが連続するトークンをまとめる\n","                entity_list = []\n","                label_head = 0  # 連続するラベルの先頭\n","                for label, group in itertools.groupby(labels):\n","                    label_tail = label_head + len(list(group)) - 1 # 連続するラベルの末尾\n","\n","                    # 予測固有表現をentitiesに格納\n","                    head = spans[label_head][0].item()\n","                    tail = spans[label_tail][1].item()\n","                    if label != 0:\n","                        entity = {'name': text[head:tail],\n","                                  'span': [head, tail],\n","                                  'type_id': label}\n","\n","                        entity_list.append(entity)\n","\n","                    label_head = label_tail + 1\n","\n","                entity_lists.append(entity_list)\n","\n","    return entity_lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2HRF3c151nK"},"outputs":[],"source":["pred = predict(test_dataloader, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJKJkv0bkeFM"},"outputs":[],"source":["def convert_type_ids_to_text(entity_list):\n","    return [{'name': entity['name'], 'span': entity['span'], 'entity_type': get_type_from_id(entity['type_id'])} for entity in entity_list]\n","\n","# 結果をランダムに確認\n","for i in range(5):\n","    index = random.randint(0, len(test_dataset) - 1)\n","\n","    print(f'テキスト　　 : {test_df[\"text\"][index]}')\n","    print(f'正解固有表現 : {convert_type_ids_to_text(test_df[\"entities\"][index])}')\n","    print(f'予測固有表現 : {convert_type_ids_to_text(pred[index])}\\n')"]},{"cell_type":"markdown","metadata":{"id":"18a_B2-_uaY-"},"source":["### 性能評価"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZd4L7SO5xYD"},"outputs":[],"source":["'''適合率、再現率、F値を計算し、モデルを評価する関数\n","'''\n","def evaluate(dataset, entities_list, predicted_entities_list, type_id=None):\n","    entities_count = 0            # 正解固有表現の個数\n","    predicted_entities_count = 0  # 予測固有表現の個数\n","    correct_count = 0             # 予測固有表現うち正解の個数\n","\n","    for entities, predicted_entities in zip(entities_list, predicted_entities_list):\n","\n","        # 引数type_idが指定された場合、そのクラスの固有表現のみを抽出\n","        if type_id:\n","            entities = [ entity for entity in entities if entity['type_id'] == type_id ]\n","            predicted_entities = [ entity for entity in predicted_entities if entity['type_id'] == type_id ]\n","\n","        # 重複固有表現をset型に変換\n","        get_span_type = lambda entity: (entity['span'][0], entity['span'][1], entity['type_id'])\n","        set_entities = set( get_span_type(entity) for entity in entities )\n","        set_entities_predicted = set( get_span_type(entity) for entity in predicted_entities )\n","\n","        # 各個数を更新\n","        entities_count += len(entities)\n","        predicted_entities_count += len(predicted_entities)\n","        correct_count += len( set_entities & set_entities_predicted )\n","\n","    precision = correct_count / predicted_entities_count    # 適合率\n","    recall = correct_count / entities_count                 # 再現率\n","    if(precision + recall != 0):\n","        f_value = 2 * precision*recall / (precision + recall) # F値\n","    else:\n","        f_value = -1\n","\n","    result = {'正解の固有表現の数': entities_count,\n","              'AIが予測した固有表現の数': predicted_entities_count,\n","              '正解数': correct_count,\n","              '適合率': precision,\n","              '再現率': recall,\n","              'F1スコア': f_value}\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2ID3OXn58Jt"},"outputs":[],"source":["evaluation_df = pd.DataFrame()\n","\n","# 各クラスの予測性能を評価\n","for key, value in id_dict.items():\n","    evaluation = evaluate(test_df, test_df['entities'], pred, type_id=value)\n","    evaluation_df[key] = evaluation.values()  # 各列に評価結果を格納\n","\n","# 全クラスの予測性能を評価\n","evaluation_all = evaluate(test_df, test_df['entities'], pred, type_id=None)\n","evaluation_df['ALL'] = evaluation_all.values()  #　全クラスの結果を末尾の列に格納\n","\n","# 行名を設定\n","evaluation_df.index = evaluation_all.keys()\n","\n","evaluation_df"]},{"cell_type":"markdown","metadata":{"id":"h1beN52WupMz"},"source":["## ファインチューニング"]},{"cell_type":"markdown","metadata":{"id":"RspdbzDatZI3"},"source":["### 学習"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0XdxJ-Lbj6M"},"outputs":[],"source":["'''モデルをファインチューニングする関数\n","'''\n","def train(model, train_dataloader, valid_dataloader, optimizer, max_epoch):\n","\n","    # モデルをGPUまたはCPUに乗せる\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    print(f'使用デバイス：{device}')\n","\n","    # ネットワークがある程度固定であれば、高速化させる\n","    torch.backends.cudnn.benchmark = True\n","\n","    train_average_loss_list = []\n","    val_average_loss_list = []\n","    history = {}\n","\n","    # epochのループ\n","    for epoch in range(max_epoch):\n","        print(f'\\nepoch [{epoch+1}/{max_epoch}]')\n","\n","        '''[Step1]学習\n","        '''\n","        model.train()\n","        sum_loss = 0.0\n","\n","        # ミニバッチを取り出す\n","        for batch_text, batch_input_ids, batch_attention_mask, batch_labels in tqdm(train_dataloader):\n","            batch_input_ids = batch_input_ids.to(device)\n","            batch_attention_mask = batch_attention_mask.to(device)\n","            batch_labels = batch_labels.to(device)\n","\n","            optimizer.zero_grad() # optimizerを初期化\n","\n","            loss, logits = model(input_ids = batch_input_ids,\n","                                 token_type_ids = None,\n","                                 attention_mask = batch_attention_mask,\n","                                 labels = batch_labels,\n","                                 return_dict = False)\n","\n","            loss.backward() # 逆伝搬\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 勾配クリッピング\n","            optimizer.step()  # 最適化\n","\n","            # 1エポックの損失の和を更新\n","            sum_loss += loss.item()\n","\n","        # 1エポックの平均損失を記録\n","        average_loss = sum_loss / len(train_dataloader)\n","        train_average_loss_list.append(average_loss)\n","\n","        '''[Step2]検証\n","        '''\n","        model.eval()\n","        sum_loss = 0.0\n","\n","        # ミニバッチを取り出す\n","        with torch.inference_mode():\n","            for batch_text, batch_input_ids, batch_attention_mask, batch_labels in (valid_dataloader):\n","                batch_input_ids = batch_input_ids.to(device)\n","                batch_attention_mask = batch_attention_mask.to(device)\n","                batch_labels = batch_labels.to(device)\n","\n","                loss, logits = model(input_ids = batch_input_ids,\n","                                    token_type_ids = None,\n","                                    attention_mask = batch_attention_mask,\n","                                    labels = batch_labels,\n","                                    return_dict = False)\n","\n","                # 1エポックの損失の和を更新\n","                sum_loss += loss.item()\n","\n","            # 1エポックの平均損失を記録\n","            average_loss = sum_loss / len(valid_dataloader)\n","            val_average_loss_list.append(average_loss)\n","\n","        print(f'train_loss: {train_average_loss_list[epoch]:.4f}, val_loss: {val_average_loss_list[epoch]:.4f}')\n","\n","    history['train_loss'] = train_average_loss_list\n","    history['val_loss'] = val_average_loss_list\n","\n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SSz5_2VXFxL"},"outputs":[],"source":["# 最適化器としてAdamを使用\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n","\n","# ファインチューニング\n","finetuned_model, history = train(model = model,\n","                                 train_dataloader = train_dataloader,\n","                                 valid_dataloader = valid_dataloader,\n","                                 optimizer = optimizer,\n","                                 max_epoch=MAX_EPOCH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPhfvVi3YfZG"},"outputs":[],"source":["# 学習曲線の表示\n","plt.figure(figsize=(4,3))\n","plt.plot(history['train_loss'],label='train', c='b')\n","plt.plot(history['val_loss'],label='val', c='r')\n","plt.title('loss')\n","plt.xticks(size=14)\n","plt.yticks(size=14)\n","plt.grid(lw=2)\n","plt.legend(fontsize=14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QiJO4HvAfiF6"},"source":["### 推論"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkVNLpbefa2X"},"outputs":[],"source":["pred_by_finetuned_model = predict(test_dataloader, finetuned_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRTED08uXNhq"},"outputs":[],"source":["# 結果をランダムに確認\n","for i in range(5):\n","    index = random.randint(0, len(test_dataset) - 1)\n","\n","    print(f'テキスト　　 : {test_df[\"text\"][index]}')\n","    print(f'正解固有表現 : {convert_type_ids_to_text(test_df[\"entities\"][index])}')\n","    print(f'予測固有表現 : {convert_type_ids_to_text(pred_by_finetuned_model[index])}\\n')"]},{"cell_type":"markdown","metadata":{"id":"Nf7O-tzHf05V"},"source":["### 性能評価"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYVGuDbxxGQg"},"outputs":[],"source":["evaluation_df = pd.DataFrame()\n","\n","# 各クラスの予測性能を評価\n","for key, value in id_dict.items():\n","    evaluation = evaluate(test_df, test_df['entities'], pred_by_finetuned_model, type_id=value)\n","    evaluation_df[key] = evaluation.values()  # 各列に評価結果を格納\n","\n","# 全クラスの予測性能を評価\n","evaluation_all = evaluate(test_df, test_df['entities'], pred_by_finetuned_model, type_id=None)\n","evaluation_df['ALL'] = evaluation_all.values()  #　全クラスの結果を末尾の列に格納\n","\n","# 行名を設定\n","evaluation_df.index = evaluation_all.keys()\n","\n","evaluation_df"]},{"cell_type":"markdown","metadata":{"id":"T4LRK39Ok0u6"},"source":["## 銀河鉄道の夜の固有表現抽出"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tsCWr2bgyCM"},"outputs":[],"source":["# 青空文庫から小説をダウンロード\n","if not os.path.exists('456_ruby_145.zip'):\n","    !wget https://www.aozora.gr.jp/cards/000081/files/456_ruby_145.zip\n","    !unzip 456_ruby_145.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyroCXXyk0u6"},"outputs":[],"source":["# ダウンロードしたtxtを確認\n","with open('gingatetsudono_yoru.txt', mode='r', encoding='shift_jis') as f:\n","    text = f.read()\n","\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzgHxPR4k0u7"},"outputs":[],"source":["#前処理\n","# ヘッダとフッタの削除\n","text = re.split(r'\\-{5,}',text)[2]\n","text = re.split(r'底本：', text)[0]\n","text = text.strip() # 連続する改行文字の削除\n","\n","text = re.sub(r'《.+?》', '', text)     # ルビを削除\n","text =text.replace('｜', '')            # ルビの付を削除\n","text = re.sub(r'［＃.+?］', '', text)   # 入力者注を削除\n","\n","text = unicodedata.normalize('NFKC', text)\n","\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-embSOZb8jAC"},"outputs":[],"source":["# データセットの作成\n","texts = text.split('\\n')\n","novel_dataset = TestDataset(texts = texts, tokenizer = tokenizer, max_length = MAX_LENGTH)\n","\n","# データローダの作成\n","novel_dataloader = DataLoader(novel_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zV6aPV5z849_"},"outputs":[],"source":["pred_for_novel = predict(novel_dataloader, finetuned_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYaqm69Nk0u7"},"outputs":[],"source":["# 各文の固有表現を取り出し\n","entity_dict = {}\n","for entity_list in pred_for_novel:\n","    # 固有表現がない場合はスキップ\n","    if len(entity_list) == 0:\n","        continue\n","\n","    # 固有表現を振り分け\n","    for entity in entity_list:\n","        entity_type = get_type_from_id(entity['type_id'])\n","        if entity_type not in entity_dict:\n","            entity_dict[entity_type] = set()\n","        entity_dict[entity_type].add(entity['name'])\n","\n","for key, value in entity_dict.items():\n","    print(f'■{key}')\n","    print(sorted(value), '\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
